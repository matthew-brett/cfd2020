---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.2.4
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Inference on slopes

In [finding lines](finding_lines), we found the line the did the best job of predicting one vector of values from another vector of values.

Our line was a slope and an intercept.  We used it to predict the Overall
Quality scores (`quality`) for courses, from the Easiness scores (`easiness`).

We chose the line to minimize the average prediction errors, and the sum of squared prediction errors.

Now we may have another question.  Can we believe the slope?   Put more
formally, is it possible that there is in fact no underlying positive or
negative slope relating  Easiness and Quality, and the slope that we found
arose because of *sampling error*.  That is, there was some random fluctuation
in the data, from course to course, and just by chance, the random fluctuation
resulted in the slope we observe.

Suggestion: It might be nice to have the something like the output of the following code here (it sometimes takes a while to run!), as an illustration of what sampling error is, in the context of line-fitting and the present example. (Just the output, not the code):


```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats

x = np.random.uniform(20,100, size = 500)
y = np.random.uniform(20,100, size = 500)


regress_random = scipy.stats.linregress(x,y)
while regress_random[3] < 0.8 :
    x = np.random.uniform(20,100, size = 500)
    y = np.random.uniform(20,100, size = 500)
    regress_random = scipy.stats.linregress(x,y)


regress_random = scipy.stats.linregress(x,y)
b0_random = regress_random[1]
b1_random = regress_random[0]
x1_random = np.linspace(min(x),max(x))
y_random = b0_random + b1_random*x1_random


print()
print('Imagine there are 500 students taking a subject. We ask each student to rate the subject for easiness and overall quality.')
print("If there were no positive or negative relationship between the easiness and quality ratings, then the scatterplot of the")
print("students' ratings would look something like this....")

plt.figure()
plt.scatter(x,y)
plt.plot(x1_random,y_random, color = 'red')
plt.axis([15, 101, 15, 101])
plt.xlabel('Easiness')
plt.ylabel('Quality')
plt.show()

df = pd.DataFrame({'x':x, 'y': y})
sample = df.sample(n = 20)
regress = scipy.stats.linregress(sample['x'],sample['y'])
while regress[2] < 0.8:
    sample = df.sample(n = 20)
    regress = scipy.stats.linregress(sample['x'],sample['y'])
    
regress_samp_err = scipy.stats.linregress(sample['x'],sample['y'])
b0_samp_err = regress_samp_err[1]
b1_samp_err = regress_samp_err[0]
x1_samp_err = np.linspace(min(sample['x']),max(sample['x']))
y_samp_err = b0_samp_err + b1_samp_err*x1_samp_err

print()
print('What if we took a sample of twenty subjects, and just by chance our sample consisted of the points shown in red below...')
plt.figure
plt.scatter(x,y)
plt.scatter(sample['x'],sample['y'], color = 'red')
plt.axis([15, 101, 15, 101])
plt.xlabel('Easiness')
plt.ylabel('Quality')
plt.show()

print()
print('Just by chance, even though there is no relationship between the two variables, the points that happened to be in our sample do show a strong relationship....')
plt.figure()
plt.scatter(sample['x'],sample['y'])
plt.plot(x1_samp_err,y_samp_err, color = 'red')
plt.axis([15, 101, 15, 101])
plt.xlabel('Easiness')
plt.ylabel('Quality')
plt.show()

print("This is sampling error, and it can be very misleading. It can lead us to believe there is a relationship between variables")
print("when in reality there isn't...")
```

This is a very similar problem to the problem of comparing means between two groups.  Like that problem, we can get at our question using a permutation test.

```{python}
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
# Make plots look a little bit more fancy
plt.style.use('fivethirtyeight')
# Print to 4 decimal places, show tiny values as 0
np.set_printoptions(precision=4, suppress=True)
import pandas as pd
```

If you are running on your laptop, you will need to download the
{download}`disciplines_SI.xlsx <../data/disciplines_SI.xlsx>` file to the
same directory as this notebook.

We read the data and get the columns:

```{python}
# Read the Excel format data file
ratings = pd.read_excel('disciplines_SI.xlsx')
# Convert Easiness and Overall Quality measures to arrays.
easiness = np.array(ratings['Easiness'])
quality = np.array(ratings['Overall Quality'])
```

The [using minimize](using_minimize) page has the function we want to minimize:

```{python}
def ss_any_line(c_s, x_values, y_values):
    c, s = c_s
    predicted = c + x_values * s
    error = y_values - predicted
    return np.sum(error ** 2)
```

Find the best intercept and slope:

```{python}
from scipy.optimize import minimize
observed_result = minimize(ss_any_line, [2.25, 0.47], args=(easiness, quality))
observed_slope = observed_result.x[1]
observed_slope
```

This is what we see, in the data we have.


What would we see, if we took another random sample of Easiness and Quality values?  We can't easily do this.  Is there a way we can estimate what this would look like?


We return to the permutation trick we used in [Brexit ages](../permutation/brexit_ages).
If there is, in fact, no relationship between Quality and Easiness, then the
slope, suggesting an association of Quality and Easiness, is just a result of
random sampling.

We can simulate that random sampling by making a new, fake (Easiness, Quality) pairing.  We do this by shuffling the Quality (or Easiness) values, to make the relationship between them random.

Then we check what slope we get.

We keep doing this, and keep checking the slope, to build up the *sampling distribution* for the slope.

Then we compare our actual slope to the sampling distribution.  If the actual slope is not a rare value from the sampling distribution, the slope may have come about because of random sampling.  If it is a rare value, random sampling is an unlikely explanation.

```{python}
# Make a copy of the quality values
shuffled_quality = quality.copy()
```

Now shuffle the copy of the Quality values, to make a fake and random pairing between the Easiness and Quality values.

```{python}
# Make a fake pairing of Easiness and Quality, by shuffling Quality
np.random.shuffle(shuffled_quality)
```

Use `minimize` to estimate the best-fit slope for this new fake pairing:

```{python}
# Estimate the slope for the fake pairing
fake_result = minimize(ss_any_line, [2.25, 0.47],
                       args=(easiness, shuffled_quality))
fake_result.x
```

Get the slope, as our first estimate from the sampling distribution:

```{python}
fake_slope = fake_result.x[1]
fake_slope
```

We need to do this many times to build up a good estimate of the sampling distribution:

```{python}
n_samples = 10000
fake_slopes = np.zeros(n_samples)
for i in np.arange(n_samples):
    np.random.shuffle(shuffled_quality)
    fake_inter, fake_slope = minimize(ss_any_line, [2.25, 0.47],
                                      args=(easiness, shuffled_quality)).x
    fake_slopes[i] = fake_slope
```

As usual, we display the sampling distribution:

```{python}
plt.hist(fake_slopes)
plt.xlabel('Fake slope values')
plt.title('Sampling distribution for slope')
```

We find the proportion of the sampling distribution that is greater than or
equal to the value we saw:

```{python}
p = np.count_nonzero(fake_slopes >= observed_slope) / n_samples
p
```

The observed slope is very unlikely, given the sampling distribution.  We conclude that the observed slope is unlikely to be the result of random sampling.
